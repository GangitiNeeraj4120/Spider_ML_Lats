{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GangitiNeeraj4120/Spider_ML_Lats/blob/main/Task1_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmurprcldVN5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import ast\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZggCe9qdhhg"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "  def __init__(self, texts):\n",
        "    self.special = ['[PAD]', '[CLS]', '[SEP]', '[UNK]']\n",
        "    self.word2idx = {w:i for i, w in enumerate(self.special)}\n",
        "    self.idx2word = {i:w for i, w in enumerate(self.special)}\n",
        "    self.build(texts)\n",
        "\n",
        "  def build(self, texts):\n",
        "    for text in texts:\n",
        "      for word in text.split():\n",
        "        if word not in self.word2idx:\n",
        "          idx = len(self.word2idx)\n",
        "          self.word2idx[word] = idx\n",
        "          self.idx2word[idx] = word\n",
        "\n",
        "  def encode(self, text, max_len):\n",
        "    tokens = ['[CLS]'] + text.split()\n",
        "    ids = [self.word2idx.get(t, self.word2idx['[UNK]']) for t in tokens]\n",
        "    ids = ids[:max_len] #Trunucate\n",
        "    ids += [0] * (max_len - len(ids)) #Padding\n",
        "    return ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5MFVLlwtDv4"
      },
      "outputs": [],
      "source": [
        "class DailyDialogDataset(Dataset):\n",
        "  def __init__(self, df, vocab, max_len=128):\n",
        "    self.dialogs = df['dialog']\n",
        "    self.labels = df['emotion']\n",
        "    self.vocab = vocab\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    dialog_list = ast.literal_eval(self.dialogs.iloc[idx])\n",
        "    text = dialog_list[0]\n",
        "    input_ids = self.vocab.encode(text, self.max_len)\n",
        "\n",
        "    emotion_str = self.labels.iloc[idx]\n",
        "    from collections import Counter\n",
        "    emotion_list = list(map(int, emotion_str.strip(\"[]\").split()))\n",
        "    label = Counter(emotion_list).most_common(1)[0][0]\n",
        "    return torch.tensor(input_ids), torch.tensor(label)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dialogs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aXnbVQOydze"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super().__init__()\n",
        "    assert d_model % num_heads == 0\n",
        "    self.num_heads = num_heads\n",
        "    self.d_k = d_model // num_heads\n",
        "\n",
        "    self.Wq = nn.Linear(d_model, d_model)\n",
        "    self.Wk = nn.Linear(d_model, d_model)\n",
        "    self.Wv = nn.Linear(d_model, d_model)\n",
        "    self.Wo = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, D = x.shape #Batch Size, Sequence length, Embedding dimensions\n",
        "\n",
        "    Q = self.Wq(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2) #Reshaping\n",
        "    K = self.Wk(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    V = self.Wv(x).view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) #Attention = Softmax(Q.K'/âˆšd_k)V\n",
        "    attn = torch.softmax(scores, dim=1) #Softmax applied alon the rows\n",
        "\n",
        "    out = torch.matmul(attn, V) #Matrix multiplication(matmul) with V\n",
        "    out = out.transpose(1, 2).contiguous().view(B, T, D) #Reshaping back to merge the heads\n",
        "\n",
        "    return self.Wo(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F84qdVbCoJeR"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_ff):\n",
        "    super().__init__()\n",
        "    self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.ff = nn.Sequential(\n",
        "        nn.Linear(d_model, d_ff),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_ff, d_model)\n",
        "    )\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.norm1(x + self.attn(x))#residual connection\n",
        "    x = self.norm2(x + self.ff(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UujzC2f15kBf"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, num_classes):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.positional = PositionalEncoding(d_model)\n",
        "    self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)])\n",
        "    self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.to(self.embedding.weight.device)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    z = self.positional(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    cls_token = x[:, 0]\n",
        "    return self.classifier(cls_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ilvq9hjWt0m0"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, max_len=512):\n",
        "    super().__init__()\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1) #pos(2i) = sin(pos/10000^(2i/d_model))\n",
        "    div = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))\n",
        "    pe[:, 0::2] = torch.sin(position*div)\n",
        "    pe[:, 1::2] = torch.cos(position*div)\n",
        "    self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TPFIDj156LL"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "all_texts = []\n",
        "for d in train_df['dialog']:\n",
        "  all_texts.append(\" \".join(eval(d)))\n",
        "\n",
        "vocab = Vocab(all_texts)\n",
        "\n",
        "dataset = DailyDialogDataset(train_df, vocab)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataset = DailyDialogDataset(test_df, vocab)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size = len(vocab.word2idx),\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    d_ff=256,\n",
        "    num_layers=2,\n",
        "    num_classes=7\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf9odPzN5tJh",
        "outputId": "8d38d97d-ad41-4cd3-a0cb-5adfa10fe698"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (embedding): Embedding(25191, 128)\n",
              "  (positional): PositionalEncoding()\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x EncoderLayer(\n",
              "      (attn): MultiHeadSelfAttention(\n",
              "        (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (Wo): Linear(in_features=128, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): Sequential(\n",
              "        (0): Linear(in_features=128, out_features=256, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=128, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDLr2yMD7VVj",
        "outputId": "354548c0-3408-4856-a5e3-e48ab0819ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 7595, 4: 3031, 6: 159, 5: 141, 1: 130, 2: 47, 3: 15})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "labels = []\n",
        "for e in train_df['emotion']:\n",
        "    labels.append(int(e.strip(\"[]\").split()[-1]))\n",
        "\n",
        "print(Counter(labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayyYBTLjA2E3"
      },
      "outputs": [],
      "source": [
        "labels = []\n",
        "for e in train_df['emotion']:\n",
        "  labels.append(int(e.strip(\"[]\").split()[-1]))\n",
        "\n",
        "from collections import Counter\n",
        "Counts = Counter(labels)\n",
        "total = sum(Counts.values())\n",
        "\n",
        "class_weights = []\n",
        "for i in range(7):\n",
        "  class_weights.append(math.log(total/(Counts[i])))\n",
        "\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxx1BnGoAl40"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tit-k3zHrEdM"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, DataLoader, device):\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "      batch_x = batch_x.to(device, non_blocking=True)\n",
        "      batch_y = batch_y.to(device, non_blocking=True)\n",
        "\n",
        "      logits = model(batch_x)\n",
        "      predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "      correct += (predictions == batch_y).sum().item()\n",
        "      total += batch_y.size(0)\n",
        "\n",
        "  accuracy = correct/total\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UtngzsNEJI7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def evaluate_with_f1(model, DataLoader, device):\n",
        "  model.eval()\n",
        "  all_preds = []\n",
        "  all_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in DataLoader:\n",
        "      x = x.to(device, non_blocking=True)\n",
        "      y = y.to(device, non_blocking=True)\n",
        "\n",
        "      logits = model(x)\n",
        "      preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "  macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "  return macro_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3stsYncpzlZn",
        "outputId": "6792a15d-087e-4d24-95b4-0e1826c303e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.519332, Test Accuracy: 0.89, Macro F1:0.261\n",
            "Epoch 2, Loss: 0.434792, Test Accuracy: 0.84, Macro F1:0.305\n",
            "Epoch 3, Loss: 0.386275, Test Accuracy: 0.85, Macro F1:0.337\n",
            "Epoch 4, Loss: 0.344056, Test Accuracy: 0.85, Macro F1:0.384\n",
            "Epoch 5, Loss: 0.323491, Test Accuracy: 0.90, Macro F1:0.419\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for x, y in train_loader:\n",
        "    x = x.to(device, non_blocking=True)\n",
        "    y = y.to(device, non_blocking=True)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = criterion(logits, y)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "  test_accuracy = evaluate(model, test_loader, device)\n",
        "  macro_f1 = evaluate_with_f1(model, test_loader, device)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Loss: {avg_loss:4f}, Test Accuracy: {test_accuracy:.2f}, Macro F1:{macro_f1:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1YZEW0k6NdFz5Q3GSoRpDxPAMWtdtW3wC",
      "authorship_tag": "ABX9TyNIe1CacZQogxYQxWGbG3PN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
